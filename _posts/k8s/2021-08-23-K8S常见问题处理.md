### Kubelet服务启动异常，Node cannot join

#### 根因分析

> 10.154.5.55:6443: connect: connection refused
> Mar 12 09:33:42 ops0008 kubelet[468453]: E0312 09:33:42.338085  468453 reflector.go:123] object-"default"/"ops-job-squirtle-config-secret": Failed to list *v1.Secret: Get https://10.154.5.55:6443/api/v1/namespaces/default/secrets?fieldSelector=metadata.name%3Dops-job-squirtle-config-secret&limit=500&resourceVersion=0: dial tcp 10.154.5.55:6443: connect: connection refused
> Mar 12 09:33:42 ops0008 kubelet[468453]: E0312 09:33:42.537905  468453 reflector.go:123] object-"kube-system"/"coredns-token-rkf5q": Failed to list *v1.Secret: Get https://10.154.5.55:6443/api/v1/namespaces/kube-system/secrets?fieldSelector=metadata.name%3Dcoredns-token-rkf5q&limit=500&resourceVersion=0: dial tcp 10.154.5.55:6443: connect: connection refused
> Mar 12 09:33:42 ops0008 kubelet[468453]: E0312 09:33:42.574294  468453 reflector.go:280] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:46: Failed to watch *v1.Pod: Get https://10.154.5.55:6443/api/v1/pods?allowWatchBookmarks=true&fieldSelector=spec.nodeName%3Dops0008&resourceVersion=8953123&timeoutSeconds=553&watch=true: dial tcp 10.154.5.55:6443: connect: connection refused
> Mar 12 09:33:42 ops0008 kubelet[468453]: W0312 09:33:42.590826  468453 kubelet_pods.go:849] Unable to retrieve pull secret default/harbor-secret for default/ops-system-auth-7c8dbf4984-cw9kc due to failed to sync secret cache: timed out waiting for the condition.  The image pull may not succeed.
> Mar 12 09:33:42 ops0008 kubelet[468453]: I0312 09:33:42.590869  468453 kuberuntime_manager.go:439] Sandbox for pod "ops-system-auth-7c8dbf4984-cw9kc_default(cb92da54-e8b1-4c40-b9e4-a09e03d6f98f)" has no IP address.  Need to start a new one
> Mar 12 09:33:42 ops0008 kubelet[468453]: W0312 09:33:42.595111  468453 cni.go:328] <font color="red" weight="stroge">CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container "202801ed5cb38ad079973727cdb083cb9b700cffb9ba75a2cf5ce5c5221aff3a"</font>
> Mar 12 09:33:42 ops0008 kubelet[468453]: 2021-03-12 09:33:42.727 [INFO][486068] plugin.go 442: Extracted identifiers ContainerID="202801ed5cb38ad079973727cdb083cb9b700cffb9ba75a2cf5ce5c5221aff3a" Node="ops0008" Orchestrator="k8s" WorkloadEndpoint="ops0008-k8s-ops--system--auth--7c8dbf4984--cw9kc-eth0"
> Mar 12 09:33:42 ops0008 kubelet[468453]: 2021-03-12 09:33:42.731 [WARNING][486068] k8s.go 447: WorkloadEndpoint does not exist in the datastore, moving forward with the clean up ContainerID="202801ed5cb38ad079973727cdb083cb9b700cffb9ba75a2cf5ce5c5221aff3a" WorkloadEndpoint="ops0008-k8s-ops--system--auth--7c8dbf4984--cw9kc-eth0"
> Mar 12 09:33:42 ops0008 kubelet[468453]: 2021-03-12 09:33:42.731 [INFO][486068] k8s.go 477: Cleaning up netns ContainerID="202801ed5cb38ad079973727cdb083cb9b700cffb9ba75a2cf5ce5c5221aff3a"
> Mar 12 09:33:42 ops0008 kubelet[468453]: 2021-03-12 09:33:42.731 [INFO][486068] k8s.go 484: Releasing IP address(es) ContainerID="202801ed5cb38ad079973727cdb083cb9b700cffb9ba75a2cf5ce5c5221aff3a"
> Mar 12 09:33:42 ops0008 kubelet[468453]: 2021-03-12 09:33:42.731 [INFO][486068] utils.go 168: Calico CNI releasing IP address ContainerID="202801ed5cb38ad079973727cdb083cb9b700cffb9ba75a2cf5ce5c5221aff3a"
> Mar 12 09:33:42 ops0008 kubelet[468453]: E0312 09:33:42.741049  468453 reflector.go:123] object-"default"/"nsre-front-stitch-nginx-conf": Failed to list *v1.ConfigMap: Get https://10.154.5.55:6443/api/v1/namespaces/default/configmaps?fieldSelector=metadata.name%3Dnsre-front-stitch-nginx-conf&limit=500&resourceVersion=0: dial tcp 10.154.5.55:6443: connect: connection refused
> Mar 12 09:33:42 ops0008 kubelet[468453]: 2021-03-12 09:33:42.776 [INFO][486086] ipam_plugin.go 299: Releasing address using handleID ContainerID="202801ed5cb38ad079973727cdb083cb9b700cffb9ba75a2cf5ce5c5221aff3a" HandleID="k8s-pod-network.202801ed5cb38ad079973727cdb083cb9b700cffb9ba75a2cf5ce5c5221aff3a" Workload="ops0008-k8s-ops--system--auth--7c8dbf4984--cw9kc-eth0"
> Mar 12 09:33:42 ops0008 kubelet[468453]: 2021-03-12 09:33:42.777 [INFO][486086] ipam.go 1166: Releasing all IPs with handle 'k8s-pod-network.202801ed5cb38ad079973727cdb083cb9b700cffb9ba75a2cf5ce5c5221aff3a'
> Mar 12 09:33:42 ops0008 kubelet[468453]: 2021-03-12 09:33:42.780 [WARNING][486086] ipam_plugin.go 306: Asked to release address but it doesn't exist. Ignoring ContainerID="202801ed5cb38ad079973727cdb083cb9b700cffb9ba75a2cf5ce5c5221aff3a" HandleID="k8s-pod-network.202801ed5cb38ad079973727cdb083cb9b700cffb9ba75a2cf5ce5c5221aff3a" Workload="ops0008-k8s-ops--system--auth--7c8dbf4984--cw9kc-eth0"
> Mar 12 09:33:42 ops0008 kubelet[468453]: 2021-03-12 09:33:42.780 [INFO][486086] ipam_plugin.go 317: Releasing address using workloadID ContainerID="202801ed5cb38ad079973727cdb083cb9b700cffb9ba75a2cf5ce5c5221aff3a" HandleID="k8s-pod-network.202801ed5cb38ad079973727cdb083cb9b700cffb9ba75a2cf5ce5c5221aff3a" Workload="ops0008-k8s-ops--system--auth--7c8dbf4984--cw9kc-eth0"
> Mar 12 09:33:42 ops0008 kubelet[468453]: 2021-03-12 09:33:42.780 [INFO][486086] ipam.go 1166: Releasing all IPs with handle 'default.ops-system-auth-7c8dbf4984-cw9kc'
> Mar 12 09:33:42 ops0008 kubelet[468453]: 2021-03-12 09:33:42.784 [INFO][486068] k8s.go 490: Teardown processing complete. ContainerID="202801ed5cb38ad079973727cdb083cb9b700cffb9ba75a2cf5ce5c5221aff3a"
> Mar 12 09:33:42 ops0008 kubelet[468453]: E0312 09:33:42.937676  468453 reflector.go:123] object-"default"/"harbor-secret": Failed to list *v1.Secret: Get https://10.154.5.55:6443/api/v1/namespaces/default/secrets?fieldSelector=metadata.name%3Dharbor-secret&limit=500&resourceVersion=0: dial tcp 10.154.5.55:6443: connect: 

#### 解决方案

After experiencing the same issue, editing /var/lib/kubelet/config.yaml to add:

修改k8s的配置文件config.yaml(默认路径为：`/var/lib/kubelet/config.yaml`)，增加如下配置

```yaml
featureGates:
  CSIMigration: false
```



### 批量强制删除异常Pod

```shell
kubectl get po --all-namespaces -o wide | grep Terminating | awk '{print $2}' | xargs kubectl  delete po --grace-period=0 --force
```

### Calico组件网络异常

> Calico对底层网络有要求，一般是需要Mac地址能够直通，不能跨二层域。



### K8s节点NotReady

#### 异常现象

1. K8s节点异常，状态为NotReady

   ```shell
   [root@k8s-master ~]# kubectl get nodes
   NAME          STATUS     ROLES    AGE     VERSION
   k8s-master    NotReady   <none>   6d17h   v1.21.4
   k8s-master2   NotReady   <none>   2d20h   v1.21.4
   k8s-node1     NotReady   <none>   5d20h   v1.21.4
   k8s-node2     NotReady   <none>   2d23h   v1.21.4
   ```

2. Kubelet异常

   ```shell
   [root@k8s-master ~]# systemctl status kubelet
   ● kubelet.service - Kubernetes Kubelet
      Loaded: loaded (/usr/lib/systemd/system/kubelet.service; enabled; vendor preset: disabled)
      Active: active (running) since 四 2021-08-26 09:38:21 CST; 2min 49s ago
    Main PID: 18719 (kubelet)
      Memory: 37.6M
      CGroup: /system.slice/kubelet.service
              └─18719 /opt/kubernetes/bin/kubelet --logtostderr=false --v=2 --log-dir=/opt/kubernetes/logs --hostname-override=k8s-master --network-plugin=cni --kubeconfig=/opt/kubernetes/cfg/kubelet.kubeconfig --boo...
   
   8月 26 09:41:09 k8s-master kubelet[18719]: E0826 09:41:09.421798   18719 kubelet.go:2291] "Error getting node" err="node \"k8s-master\" not found"
   8月 26 09:41:09 k8s-master kubelet[18719]: E0826 09:41:09.523374   18719 kubelet.go:2291] "Error getting node" err="node \"k8s-master\" not found"
   8月 26 09:41:09 k8s-master kubelet[18719]: E0826 09:41:09.624278   18719 kubelet.go:2291] "Error getting node" err="node \"k8s-master\" not found"
   8月 26 09:41:09 k8s-master kubelet[18719]: E0826 09:41:09.724654   18719 kubelet.go:2291] "Error getting node" err="node \"k8s-master\" not found"
   8月 26 09:41:09 k8s-master kubelet[18719]: E0826 09:41:09.824910   18719 kubelet.go:2291] "Error getting node" err="node \"k8s-master\" not found"
   8月 26 09:41:09 k8s-master kubelet[18719]: E0826 09:41:09.926380   18719 kubelet.go:2291] "Error getting node" err="node \"k8s-master\" not found"
   8月 26 09:41:10 k8s-master kubelet[18719]: E0826 09:41:10.027721   18719 kubelet.go:2291] "Error getting node" err="node \"k8s-master\" not found"
   8月 26 09:41:10 k8s-master kubelet[18719]: E0826 09:41:10.128469   18719 kubelet.go:2291] "Error getting node" err="node \"k8s-master\" not found"
   8月 26 09:41:10 k8s-master kubelet[18719]: E0826 09:41:10.229202   18719 kubelet.go:2291] "Error getting node" err="node \"k8s-master\" not found"
   8月 26 09:41:10 k8s-master kubelet[18719]: E0826 09:41:10.329554   18719 kubelet.go:2291] "Error getting node" err="node \"k8s-master\" not found"
   ```

3. journalctl 查看日志

   ```shell
   [root@k8s-master ~]# journalctl -u kubelet
   
   8月 26 09:20:52 k8s-master kubelet[2233]: E0826 09:20:52.717758    2233 controller.go:144] failed to ensure lease exists, will retry in 400ms, error: Get "https://192.168.56.108:16443/apis/coordination.k8s.io/v1/name
   spaces/kube-node-lease/leases/k8s-master?timeout=10s": x509: certificate is valid for 10.0.0.1, 127.0.0.1, 192.168.56.101, 192.168.56.102, 192.168.56.103, 192.168.56.104, 192.168.56.105, 192.168.56.106, 192.168.56.10
   7, not 192.168.56.108
   ```

#### 根因分析

> 从`journalctl -u kubelet`的日志中看出，192.168.56.108不在证书许可范围内，原来我们配置证书内容时，设置的IP192.168.56.101~192.168.56.107，但是在配置`Keepalived`时，设置的VIP是192.168.56.108，故导致证书认证失败。

#### 解决方案

1. 修改 /etc/keepalived/keepalived.conf

   ```shell
   [root@k8s-master ~]# sed -i 's#192.168.56.108#192.168.56.107#' /etc/keepalived/keepalived.conf
   [root@k8s-master ~]# systemctl restart keepalived
   ```

2. 修改K8s的配置文件里设置的apiserver的地址

   ```shell
   [root@k8s-master ~]# sed -i 's#192.168.56.108#192.168.56.107#' /opt/kubernetes/cfg/* 
   [root@k8s-master ~]# systemctl restart kubelet 
   [root@k8s-master ~]# systemctl restart kube-proxy
   ```

#### 修复结果

```shell
[root@k8s-master2 ~]# kubectl get nodes
NAME          STATUS   ROLES    AGE     VERSION
k8s-master    Ready    <none>   6d17h   v1.21.4
k8s-master2   Ready    <none>   2d20h   v1.21.4
k8s-node1     Ready    <none>   5d20h   v1.21.4
k8s-node2     Ready    <none>   2d23h   v1.21.4
```

